# -*- coding: utf-8 -*-
"""Auto_Crypto_Btc_Concat

Automatically generated by Colab.

# Data

This section has been used to merge the Bitcoin data.

## ğŸ”” Disclaimer | Yasal UyarÄ±

**English:**

ğŸš¨ This project is strictly for **educational use only**.  
- It does **not provide financial or investment advice**.  
- The developer is **not responsible** for any financial loss.  
- Use **only in test environments** or simulations.

**TÃ¼rkÃ§e:**

â—ï¸Bu proje **yatÄ±rÄ±m tavsiyesi deÄŸildir**.  
- YalnÄ±zca **eÄŸitim ve araÅŸtÄ±rma** amacÄ±yla geliÅŸtirilmiÅŸtir.  
- Finansal kayÄ±plardan geliÅŸtirici **sorumlu deÄŸildir**.  
- Sadece **test ortamlarÄ±nda** ya da **simÃ¼lasyonlarda** kullanÄ±nÄ±z.

---

âœ… LÃ¼tfen bu projeyi kullanmadan Ã¶nce yukarÄ±daki uyarÄ±larÄ± dikkatlice okuyunuz.
"""

import warnings
warnings.filterwarnings("ignore")


import modin.pandas as pd
import dask.dataframe as dd

data_18 = dd.read_csv("btc_201801010000_201901010000.csv")
data_19 = dd.read_csv("btc_201901010000_202001010000.csv")
data_20 = dd.read_csv("btc_202001010000_202101010000.csv")
data_21 = dd.read_csv("btc_202101010000_202201010000.csv")
data_22 = dd.read_csv("btc_202201010000_202301010000.csv")
data_23 = dd.read_csv("btc_202301010000_202401010000.csv")
data_24 = dd.read_csv("btc_202401010000_202501010000.csv")
data_25 = dd.read_csv("btc_202501010000_202505090000.csv")

data_18 = data_18.compute()
data_19 = data_19.compute()
data_20 = data_20.compute()
data_21 = data_21.compute()
data_22 = data_22.compute()
data_23 = data_23.compute()
data_24 = data_24.compute()
data_25 = data_25.compute()

data_18

data_19

data_20.info()

data_18_to_24 = pd.concat([data_18,data_19,data_20,data_21,data_22,data_23,data_24,data_25],ignore_index=True)

data_18_to_24

data_18_to_24.drop_duplicates(inplace=True)

data_18_to_24.info()

data_18_to_24.to_csv("btc_data.csv",index=False)

data_18_to_24['timestamp'] = pd.to_datetime(data_18_to_24['timestamp'])

data_18_to_24['year'] = data_18_to_24['timestamp'].dt.year
data_18_to_24['month'] = data_18_to_24['timestamp'].dt.month
data_18_to_24['day'] = data_18_to_24['timestamp'].dt.day
data_18_to_24['hour'] = data_18_to_24['timestamp'].dt.hour
data_18_to_24['minute'] = data_18_to_24['timestamp'].dt.minute

data_18_to_24['target'] = data_18_to_24['close'].shift(-1)
for i in range(1, 16):
    data_18_to_24[f'lag_{i}'] = data_18_to_24['close'].shift(i)

data_18_to_24.dropna(inplace=True)

data_18_to_24.isnull().sum()

data_18_to_24.info()

data_18_to_24.to_csv("btc_data_min.csv",index=False)

"""# In the section below, I extracted data for every 30 minutes, but do not use it directly because the lags and targets are different. The data in between has been deleted."""

filtered_data = data_18_to_24[data_18_to_24['timestamp'].dt.minute.isin([0, 30])]

filtered_data.to_csv("btc_data_30min.csv",index=False)